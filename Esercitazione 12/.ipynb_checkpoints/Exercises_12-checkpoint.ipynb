{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee65e5b2",
   "metadata": {},
   "source": [
    "AdaGrad (adaptive gradient) è un metodo pubblicato nel 2011[21][22] che usa un tasso di apprendimento indipendente per ogni parametro. Intuitivamente, questo metodo usa tassi di apprendimento più elevati per parametri con una distribuzione più sparsa, e più lenti per i parametri con distribuzione più densa. AdaGrad spesso converge più rapidamente rispetto a SGD ordinaria, soprattutto quando la distribuzione dei dati è sparsa e i parametri sparsi hanno maggior contenuto informativo, che si verifica ad esempio in problemi di elaborazione del linguaggio naturale o riconoscimento automatico del contenuto di immagini.[21]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247a870",
   "metadata": {},
   "source": [
    "uso Adagrad con parametri standard e peggioro nettamente le prestazioni con quasi il doppio di val_loss. Probabilmente i datinon sono così randomici come dovrebbe funzionare al meglio e i parametri non sono settati ottimamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af2e34",
   "metadata": {},
   "source": [
    "Adaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
    "Root Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517043f",
   "metadata": {},
   "source": [
    "Con Rms parto passo 0.23 e imparo velocemente il rumore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f8a06",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "per 10 epoche\n",
    "\n",
    "con SGD Val_test=0.10888\n",
    "cono Adagrad val_test=0.25189\n",
    "con RMs (dopo 5 epoche) val_test=0.1672\n",
    "con RMs (dopo 10 epoche) val_test=0.2165"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
